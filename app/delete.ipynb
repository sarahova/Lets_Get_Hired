{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "# import time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Importing modules\n",
    "import pandas as pd\n",
    "# from wordcloud import WordCloud\n",
    "# Load the regular expression library\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from skills import skills\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Creating a list of stop words and adding custom stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "##Creating a list of custom stopwords\n",
    "# new_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\", 'business', 'client', 'need']\n",
    "# stop_words = stop_words.union(new_words)\n",
    "\n",
    "\n",
    "def get_total_jobs(url):\n",
    "    page = requests.get(url)\n",
    "    soup=bs(page.text, 'lxml')\n",
    "    total=soup.find(id='searchCountPages').text\n",
    "    total = total[total.find('of')+3:total.find('jobs')-1]\n",
    "    total = total.replace(',','')\n",
    "    return int(total)\n",
    "\n",
    "\n",
    "def get_all_links_in_page(url):\n",
    "    sublist = []\n",
    "    page = requests.get(url)\n",
    "    soup=bs(page.text, 'lxml')\n",
    "    page_list=soup.find('ul', class_='pagination-list')\n",
    "    try:\n",
    "      page_len=len(page_list.find_all('li'))\n",
    "    except:\n",
    "      print('something went wrong')\n",
    "\n",
    "    job_listings=soup.find_all('div', attrs={'class': 'jobsearch-SerpJobCard'})\n",
    "    for listing in job_listings:\n",
    "        jk=listing['data-jk']\n",
    "        job_site=f'https://ca.indeed.com/viewjob?jk={jk}'\n",
    "        sublist.append(job_site)\n",
    "    \n",
    "    return sublist\n",
    "\n",
    "\n",
    "\n",
    "def get_all_url_from_job(page, position, location, timeline):\n",
    "    start = page*10\n",
    "    url=\"https://ca.indeed.com/jobs?q=\"+position+\"&l=\"+location+\"&fromage=\"+timeline+'&start='+str(start)\n",
    "    sub_list = get_all_links_in_page(url)\n",
    "    return sub_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scrape(url):\n",
    "  try:\n",
    "    response=requests.get(url)\n",
    "    soup=bs(response.text, 'lxml')\n",
    "    text=soup.find('div', id='jobDescriptionText').text.strip()\n",
    "  except:\n",
    "    text=''  \n",
    "    print(url)\n",
    "    print('empty')\n",
    "  return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from PIL import Image\n",
    "# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "# import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "\n",
    "# wordcloud = WordCloud(width=800, height=400).generate(str(corpus))\n",
    "# plt.figure( figsize=(20,10), facecolor='k')\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "#Most frequently occuring words\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items() if word in skills]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "#Most frequently occuring Bi-grams\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items() if word in skills]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "#Convert most freq words to dataframe for plotting bar plot\n",
    "\n",
    "# #Barplot of most freq Bi-grams\n",
    "# import seaborn as sns\n",
    "# sns.set(rc={'figure.figsize':(13,8)})\n",
    "# # plt.title(position + '\\n total jobs found: {}'.format(total_jobs))\n",
    "# plt.title('top skills needed for {}'.format(position))\n",
    "\n",
    "# h=sns.barplot(x=\"Skill\", y=\"Freq\", data=top_df)\n",
    "# h.set_xticklabels(h.get_xticklabels(), rotation=45)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_all(string):\n",
    "\n",
    "    multi=False\n",
    "\n",
    "    position=string\n",
    "\n",
    "    job_sites={}\n",
    "\n",
    "\n",
    "    location='Toronto'\n",
    "    timeline='14'\n",
    "    start=0\n",
    "\n",
    "    job_list=[]\n",
    " \n",
    "\n",
    "\n",
    "    #get first page info\n",
    "    url=\"https://ca.indeed.com/jobs?q=\"+position+\"&l=\"+location+\"&fromage=\"+timeline+'&start='+str(start)\n",
    "\n",
    "    total_jobs = get_total_jobs(url)\n",
    "\n",
    "    if multi:\n",
    "        # multiprocessing\n",
    "        p = Pool(workers)\n",
    "        prelim_job_list = p.map(get_all_url_from_job, list(range(0,int(total_jobs/10))))\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "\n",
    "    else:\n",
    "        #single_process\n",
    "        prelim_job_list = []\n",
    "        for p in list(range(0,int(total_jobs/10))):\n",
    "            prelim_job_list.append(get_all_url_from_job(p, position, location, timeline))\n",
    "\n",
    "    flatten = [item for sublist in prelim_job_list for item in sublist]\n",
    "    job_list = list(set(flatten))\n",
    "\n",
    "\n",
    "    if multi:\n",
    "        p = Pool(workers)\n",
    "        corpus = p.map(scrape, job_list)\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "\n",
    "    else:\n",
    "        corpus = []\n",
    "        for p in job_list:\n",
    "            corpus.append(scrape(p))\n",
    "\n",
    "\n",
    "    # cleaning the raw text\n",
    "    corpus = [re.sub('[,\\.!?()+]', '', t) for t in corpus]\n",
    "    corpus = [t.lower() for t in corpus]\n",
    "    corpus = [t.replace('\\n',' ') for t in corpus]\n",
    "\n",
    "    # text_list = [p_text,p_text]\n",
    "\n",
    "\n",
    "    for i in range(0,len(corpus)):\n",
    "\n",
    "        word_tokens = word_tokenize(corpus[i])\n",
    "\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "\n",
    "        filtered_sentence = [] \n",
    "\n",
    "        for w in word_tokens: \n",
    "          if w not in stop_words: \n",
    "              filtered_sentence.append(w) \n",
    "        filtered_sentence_join = ' '.join(filtered_sentence)\n",
    "\n",
    "        corpus[i] = filtered_sentence_join\n",
    "\n",
    "\n",
    "    cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(2,3))\n",
    "    X=cv.fit_transform(corpus)\n",
    "\n",
    "    top_words = get_top_n_words(corpus, n=10)\n",
    "    top_df = pd.DataFrame(top_words)\n",
    "    top_df.columns=[\"Skill\", \"Freq\"]\n",
    "\n",
    "    top2_words = get_top_n2_words(corpus, n=10)\n",
    "    top2_df = pd.DataFrame(top2_words)\n",
    "    top2_df.columns=[\"Skill\", \"Freq\"]\n",
    "\n",
    "    top_df = top_df.append(top2_df)\n",
    "    top_df.sort_values('Freq', ascending=True, inplace=True)\n",
    "    print(top_df.to_json(orient='records'))\n",
    "    return  top_df.to_dict(orient='records')\n",
    "    \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"Skill\":\"data mining\",\"Freq\":2},{\"Skill\":\"relational database\",\"Freq\":2},{\"Skill\":\"deep learning\",\"Freq\":5},{\"Skill\":\"statistical analysis\",\"Freq\":6},{\"Skill\":\"data analysis\",\"Freq\":6},{\"Skill\":\"excel\",\"Freq\":9},{\"Skill\":\"hadoop\",\"Freq\":10},{\"Skill\":\"spark\",\"Freq\":10},{\"Skill\":\"writing\",\"Freq\":11},{\"Skill\":\"big data\",\"Freq\":12},{\"Skill\":\"modeling\",\"Freq\":15},{\"Skill\":\"sql\",\"Freq\":16},{\"Skill\":\"reports\",\"Freq\":17},{\"Skill\":\"cloud\",\"Freq\":21},{\"Skill\":\"python\",\"Freq\":24},{\"Skill\":\"engineering\",\"Freq\":25},{\"Skill\":\"machine learning\",\"Freq\":46}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skill</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data mining</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relational database</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deep learning</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>statistical analysis</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data analysis</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>excel</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hadoop</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spark</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>writing</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>big data</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>modeling</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sql</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reports</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cloud</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>python</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>engineering</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine learning</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Skill  Freq\n",
       "6           data mining     2\n",
       "5   relational database     2\n",
       "4         deep learning     5\n",
       "3  statistical analysis     6\n",
       "2         data analysis     6\n",
       "9                 excel     9\n",
       "7                hadoop    10\n",
       "8                 spark    10\n",
       "6               writing    11\n",
       "1              big data    12\n",
       "5              modeling    15\n",
       "4                   sql    16\n",
       "3               reports    17\n",
       "2                 cloud    21\n",
       "1                python    24\n",
       "0           engineering    25\n",
       "0      machine learning    46"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = run_all('data scientist')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Freq': 2, 'Skill': 'data mining'},\n",
       " {'Freq': 2, 'Skill': 'relational database'},\n",
       " {'Freq': 5, 'Skill': 'deep learning'},\n",
       " {'Freq': 6, 'Skill': 'statistical analysis'},\n",
       " {'Freq': 6, 'Skill': 'data analysis'},\n",
       " {'Freq': 9, 'Skill': 'excel'},\n",
       " {'Freq': 10, 'Skill': 'hadoop'},\n",
       " {'Freq': 10, 'Skill': 'spark'},\n",
       " {'Freq': 11, 'Skill': 'writing'},\n",
       " {'Freq': 12, 'Skill': 'big data'},\n",
       " {'Freq': 15, 'Skill': 'modeling'},\n",
       " {'Freq': 16, 'Skill': 'sql'},\n",
       " {'Freq': 17, 'Skill': 'reports'},\n",
       " {'Freq': 21, 'Skill': 'cloud'},\n",
       " {'Freq': 24, 'Skill': 'python'},\n",
       " {'Freq': 25, 'Skill': 'engineering'},\n",
       " {'Freq': 46, 'Skill': 'machine learning'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
